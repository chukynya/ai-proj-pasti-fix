{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Disease Prediction - Model Comparison Analysis\n",
    "\n",
    "This notebook demonstrates the comparison of multiple classification algorithms for disease prediction using blood test data.\n",
    "\n",
    "## Project Overview\n",
    "- **Objective**: Compare classification models for disease prediction\n",
    "- **Focus**: Optimize recall score to minimize false negatives\n",
    "- **Data**: Blood test parameters\n",
    "- **Models**: Random Forest, XGBoost, SVM, Logistic Regression, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Import project modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data_preprocessing import load_and_preprocess_data\n",
    "from model_comparison import compare_classification_models\n",
    "from evaluation import evaluate_multiple_models\n",
    "from visualization import create_comprehensive_visualization_report\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists('../data/sample_blood_data.csv'):\n",
    "    print(\"Generating sample dataset...\")\n",
    "    from generate_sample_data import save_sample_datasets\n",
    "    save_sample_datasets(output_dir='../data')\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "X_train, X_test, y_train, y_test, feature_names, scaler = load_and_preprocess_data(\n",
    "    '../data/sample_blood_data.csv',\n",
    "    target_column='Disease',\n",
    "    test_size=0.2,\n",
    "    apply_balancing=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare classification models\n",
    "print(\"Comparing classification models...\")\n",
    "comparison_results = compare_classification_models(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    cv_folds=5,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display cross-validation results\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(\"=\" * 50)\n",
    "cv_results = comparison_results['cv_results']\n",
    "display(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display test results\n",
    "print(\"Test Set Results:\")\n",
    "print(\"=\" * 30)\n",
    "test_results = comparison_results['test_results']\n",
    "display(test_results.sort_values('Recall', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best model information\n",
    "best_model_name = comparison_results['best_model_name']\n",
    "best_model = comparison_results['best_model']\n",
    "best_recall = comparison_results['best_recall']\n",
    "\n",
    "print(f\"Best Model by Recall Score:\")\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(f\"Recall Score: {best_recall:.4f}\")\n",
    "\n",
    "# Detailed evaluation\n",
    "from evaluation import create_evaluation_report\n",
    "\n",
    "evaluation_report = create_evaluation_report(\n",
    "    best_model, X_test, y_test,\n",
    "    model_name=best_model_name,\n",
    "    feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for the best model\n",
    "feature_importance = evaluation_report['feature_importance']\n",
    "\n",
    "if feature_importance is not None:\n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(\"=\" * 35)\n",
    "    display(feature_importance)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(feature_importance)), feature_importance['importance'])\n",
    "    plt.yticks(range(len(feature_importance)), feature_importance['feature'])\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.title(f'Feature Importance - {best_model_name}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(f\"Feature importance not available for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization report\n",
    "models_dict = comparison_results['trained_models']\n",
    "\n",
    "# Get feature importance for visualizable models\n",
    "feature_importance_dict = {}\n",
    "for model_name, model in models_dict.items():\n",
    "    try:\n",
    "        if hasattr(model, 'feature_importances_') or hasattr(model, 'coef_'):\n",
    "            from evaluation import ModelEvaluator\n",
    "            evaluator = ModelEvaluator(model, model_name)\n",
    "            importance = evaluator.get_feature_importance(feature_names, top_n=10)\n",
    "            feature_importance_dict[model_name] = importance\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Create visualization report\n",
    "figures = create_comprehensive_visualization_report(\n",
    "    models_dict, test_results, X_test, y_test,\n",
    "    feature_importance_dict, save_directory=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table with key metrics\n",
    "summary_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "summary_df = test_results[summary_metrics].round(4)\n",
    "\n",
    "print(\"Model Performance Summary (Test Set):\")\n",
    "print(\"=\" * 45)\n",
    "display(summary_df)\n",
    "\n",
    "# Highlight best performers\n",
    "print(\"\\nTop 3 Models by Recall Score:\")\n",
    "print(\"-\" * 35)\n",
    "top_3_recall = summary_df.sort_values('Recall', ascending=False).head(3)\n",
    "for idx, (model, row) in enumerate(top_3_recall.iterrows(), 1):\n",
    "    print(f\"{idx}. {model}: Recall = {row['Recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Medical Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medical interpretation of results\n",
    "print(\"MEDICAL INTERPRETATION:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "best_metrics = evaluation_report['evaluation_metrics']\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Recall (Sensitivity): {best_metrics['recall']:.4f}\")\n",
    "print(f\"Precision (PPV): {best_metrics['precision']:.4f}\")\n",
    "print(f\"Specificity: {best_metrics['specificity']:.4f}\")\n",
    "print(f\"NPV: {best_metrics['npv']:.4f}\")\n",
    "print()\n",
    "\n",
    "# Clinical significance\n",
    "recall = best_metrics['recall']\n",
    "precision = best_metrics['precision']\n",
    "\n",
    "print(\"Clinical Significance:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "if recall >= 0.9:\n",
    "    print(\"✓ EXCELLENT RECALL: Model catches 90%+ of disease cases\")\n",
    "    print(\"  → Very good for screening and early detection\")\n",
    "elif recall >= 0.8:\n",
    "    print(\"✓ GOOD RECALL: Model catches 80%+ of disease cases\")\n",
    "    print(\"  → Acceptable for most diagnostic applications\")\n",
    "else:\n",
    "    print(\"⚠ MODERATE RECALL: Model misses some disease cases\")\n",
    "    print(\"  → May need improvement for clinical use\")\n",
    "\n",
    "if precision >= 0.8:\n",
    "    print(\"✓ HIGH PRECISION: Few false alarms\")\n",
    "    print(\"  → Reduces unnecessary worry and follow-up tests\")\n",
    "elif precision >= 0.6:\n",
    "    print(\"○ MODERATE PRECISION: Some false alarms\")\n",
    "    print(\"  → Acceptable but may cause some unnecessary concern\")\n",
    "else:\n",
    "    print(\"⚠ LOW PRECISION: Many false alarms\")\n",
    "    print(\"  → May cause excessive worry and unnecessary tests\")\n",
    "\n",
    "# Cost-benefit analysis\n",
    "cost_benefit = evaluation_report['cost_benefit_analysis']\n",
    "if cost_benefit:\n",
    "    print(f\"\\nCost-Benefit Analysis:\")\n",
    "    print(f\"Net Benefit per Case: ${cost_benefit['net_benefit_per_case']:.2f}\")\n",
    "    if cost_benefit['net_benefit'] > 0:\n",
    "        print(\"✓ Model provides positive economic value\")\n",
    "    else:\n",
    "        print(\"⚠ Model may not be cost-effective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Recommendations for Team Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "1. **Best Performing Model**: Based on recall optimization\n",
    "2. **Recall Score Achievement**: Target of >90% recall for medical screening\n",
    "3. **Feature Importance**: Most predictive blood parameters identified\n",
    "4. **Model Comparison**: Comprehensive evaluation across multiple algorithms\n",
    "\n",
    "### Next Steps for Team\n",
    "\n",
    "**Person 1 (Data Preprocessing)**: \n",
    "- Explore additional feature engineering techniques\n",
    "- Investigate different imputation strategies\n",
    "- Analyze feature correlations and interactions\n",
    "\n",
    "**Person 2 (Model Implementation)**:\n",
    "- Implement ensemble methods\n",
    "- Try additional algorithms (Neural Networks, etc.)\n",
    "- Explore model stacking approaches\n",
    "\n",
    "**Person 3 (Hyperparameter Tuning)**:\n",
    "- Perform extensive hyperparameter optimization\n",
    "- Use advanced optimization techniques (Optuna, Bayesian)\n",
    "- Focus on recall-optimized tuning\n",
    "\n",
    "**Person 4 (Evaluation & Documentation)**:\n",
    "- Create detailed performance analysis\n",
    "- Develop clinical interpretation guidelines\n",
    "- Prepare presentation materials\n",
    "\n",
    "### Project Deliverables\n",
    "\n",
    "- [ ] Complete model comparison report\n",
    "- [ ] Best model with >90% recall\n",
    "- [ ] Feature importance analysis\n",
    "- [ ] Clinical interpretation guide\n",
    "- [ ] Interactive visualization dashboard\n",
    "- [ ] Team presentation\n",
    "\n",
    "### Success Metrics\n",
    "\n",
    "- **Primary**: Recall Score > 0.90\n",
    "- **Secondary**: Balanced precision and specificity\n",
    "- **Tertiary**: Model interpretability and clinical relevance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}